import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration
import cv2
import numpy as np
from tensorflow.keras.models import load_model
import av
import time
from PIL import Image
import os
import gdown
import threading
import queue
from pathlib import Path
import logging

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Constants
MODEL_PATH = "best_model1.h5 (1).keras"
GOOGLE_DRIVE_URL = "https://drive.google.com/file/d/1zv4QRkSa8gKx4U3b5Ox0uL0ilUMFk1b8/view?usp=sharing"
EMOTION_LABELS = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']
EMOTION_COLORS = {
    'angry': (0, 0, 255),      # Red
    'disgust':(0, 255, 255),    # Yellow
    'fear': (128, 0, 128),     # Purple
    'happy': (0, 128, 0),    # Green
    'sad': (255, 0, 0),        # Blue
    'surprise': (255, 165, 0), # Orange
    'neutral': (128, 128, 128) # Gray
}

class ModelManager:
    """Qu·∫£n l√Ω t·∫£i v√† cache m√¥ h√¨nh"""
    
    @staticmethod
    @st.cache_resource
    def load_emotion_model():
        """T·∫£i m√¥ h√¨nh v·ªõi error handling t·ªët h∆°n"""
        model_path = Path(MODEL_PATH)
        
        # Ki·ªÉm tra file t·ªìn t·∫°i v√† t·∫£i tr·ª±c ti·∫øp
        if model_path.exists():
            try:
                model = load_model(str(model_path), compile=False)
                st.success(f"‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng t·ª´: {model_path}")
                logger.info(f"Model loaded successfully from: {model_path}")
                return model
            except Exception as e:
                st.error(f"‚ùå L·ªói khi t·∫£i m√¥ h√¨nh t·ª´ file c√≥ s·∫µn: {e}")
                logger.error(f"Model loading error: {e}")
                # X√≥a file l·ªói
                try:
                    model_path.unlink()
                except:
                    pass
        
        # T·∫£i t·ª´ Google Drive
        return ModelManager._download_from_drive()
    
    @staticmethod
    def _download_from_drive():
        """T·∫£i m√¥ h√¨nh t·ª´ Google Drive"""
        with st.spinner("üîÑ ƒêang t·∫£i m√¥ h√¨nh t·ª´ Google Drive..."):
            try:
                gdown.download(GOOGLE_DRIVE_URL, MODEL_PATH, quiet=False)
                if Path(MODEL_PATH).exists():
                    model = load_model(MODEL_PATH, compile=False)
                    st.success("‚úÖ T·∫£i m√¥ h√¨nh t·ª´ Google Drive th√†nh c√¥ng!")
                    return model
                else:
                    raise FileNotFoundError("Kh√¥ng th·ªÉ t·∫£i file t·ª´ Google Drive.")
            except Exception as e:
                st.error(f"‚ùå L·ªói khi t·∫£i t·ª´ Google Drive: {e}")
                return ModelManager._manual_upload()
    
    @staticmethod
    def _manual_upload():
        """Cho ph√©p t·∫£i m√¥ h√¨nh th·ªß c√¥ng"""
        st.subheader("üì§ T·∫£i m√¥ h√¨nh th·ªß c√¥ng")
        st.info("Vui l√≤ng t·∫£i file m√¥ h√¨nh (.h5 ho·∫∑c .keras) ƒë·ªÉ ti·∫øp t·ª•c.")
        
        uploaded_model = st.file_uploader(
            "Ch·ªçn file m√¥ h√¨nh", 
            type=["h5", "keras"],
            help="File m√¥ h√¨nh ph·∫£i c√≥ ƒë·ªãnh d·∫°ng .h5 ho·∫∑c .keras"
        )
        
        if uploaded_model is not None:
            try:
                with open(MODEL_PATH, "wb") as f:
                    f.write(uploaded_model.getbuffer())
                
                model = load_model(MODEL_PATH, compile=False)
                st.success("‚úÖ File m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i l√™n v√† kh·ªüi t·∫°o th√†nh c√¥ng!")
                st.rerun()  # Refresh app
                return model
            except Exception as e:
                st.error(f"‚ùå L·ªói khi l∆∞u/t·∫£i file m√¥ h√¨nh: {e}")
                return None
        
        return None

class ImageProcessor:
    """X·ª≠ l√Ω h√¨nh ·∫£nh v√† d·ª± ƒëo√°n c·∫£m x√∫c"""
    
    @staticmethod
    def preprocess_face(face_img):
        """Ti·ªÅn x·ª≠ l√Ω ·∫£nh khu√¥n m·∫∑t v·ªõi error handling"""
        try:
            if face_img is None or face_img.size == 0:
                return None
                
            # Chuy·ªÉn ƒë·ªïi m√†u s·∫Øc
            if len(face_img.shape) == 3 and face_img.shape[2] == 3:
                face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)
            
            # C·∫£i thi·ªán ƒë·ªô t∆∞∆°ng ph·∫£n
            lab = cv2.cvtColor(face_img, cv2.COLOR_RGB2LAB)
            l, a, b = cv2.split(lab)
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            l = clahe.apply(l)
            lab = cv2.merge((l, a, b))
            face_img = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
            
            # Resize v√† normalize
            face_img = cv2.resize(face_img, (299, 299))
            face_img = face_img.astype('float32')
            face_img = (face_img - 127.5) / 127.5
            face_img = np.expand_dims(face_img, axis=0)
            
            return face_img
        except Exception as e:
            logger.error(f"Preprocessing error: {e}")
            return None
    
    @staticmethod
    def predict_emotion(face_img, model):
        """D·ª± ƒëo√°n c·∫£m x√∫c v·ªõi confidence threshold"""
        try:
            if model is None:
                return None, 0.0
                
            processed_img = ImageProcessor.preprocess_face(face_img)
            if processed_img is None:
                return None, 0.0
            
            predictions = model.predict(processed_img, verbose=0)
            predicted_class = np.argmax(predictions[0])
            confidence = float(predictions[0][predicted_class])
            
            # Ch·ªâ tr·∫£ v·ªÅ k·∫øt qu·∫£ n·∫øu confidence > threshold
            if confidence > 0.3:  # Threshold ƒë·ªÉ l·ªçc prediction y·∫øu
                return EMOTION_LABELS[predicted_class], confidence
            else:
                return None, confidence
                
        except Exception as e:
            logger.error(f"Prediction error: {e}")
            return None, 0.0

class OptimizedVideoProcessor(VideoProcessorBase):
    """Video processor ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a"""
    
    def __init__(self, model):
        self.model = model
        self.face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        
        # Performance tracking
        self.fps_counter = 0
        self.fps_start_time = time.time()
        self.current_fps = 0
        
        # Prediction caching
        self.last_prediction = None
        self.last_prediction_time = 0
        self.prediction_interval = 0.3  # Gi·∫£m xu·ªëng ƒë·ªÉ responsive h∆°n
        
        # Frame skipping for better performance
        self.frame_count = 0
        self.process_every_n_frames = 3  # X·ª≠ l√Ω m·ªói 3 frame
        
        logger.info("VideoProcessor initialized successfully")
    
    def recv(self, frame):
        try:
            img = frame.to_ndarray(format="bgr24")
            self.frame_count += 1
            current_time = time.time()
            
            # T√≠nh FPS
            self.fps_counter += 1
            if current_time - self.fps_start_time >= 1.0:
                self.current_fps = self.fps_counter / (current_time - self.fps_start_time)
                self.fps_counter = 0
                self.fps_start_time = current_time
            
            # Skip frames ƒë·ªÉ t·ªëi ∆∞u performance
            should_process = (self.frame_count % self.process_every_n_frames == 0)
            should_predict = (current_time - self.last_prediction_time) >= self.prediction_interval
            
            output_img = img.copy()
            
            if should_process:
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                
                # Detect faces v·ªõi parameters t·ªëi ∆∞u
                faces = self.face_cascade.detectMultiScale(
                    gray, 
                    scaleFactor=1.1, 
                    minNeighbors=3, 
                    minSize=(60, 60),  # TƒÉng minSize ƒë·ªÉ tr√°nh false positive
                    flags=cv2.CASCADE_SCALE_IMAGE
                )
                
                if len(faces) > 0 and should_predict:
                    # Ch·ªçn khu√¥n m·∫∑t l·ªõn nh·∫•t (g·∫ßn camera nh·∫•t)
                    largest_face = max(faces, key=lambda face: face[2] * face[3])
                    x, y, w, h = largest_face
                    
                    # M·ªü r·ªông v√πng face m·ªôt ch√∫t
                    margin = int(max(w, h) * 0.1)
                    face_x = max(0, x - margin)
                    face_y = max(0, y - margin)
                    face_w = min(img.shape[1] - face_x, w + 2 * margin)
                    face_h = min(img.shape[0] - face_y, h + 2 * margin)
                    
                    face_img = img[face_y:face_y+face_h, face_x:face_x+face_w]
                    
                    if face_img.size > 0:
                        emotion, confidence = ImageProcessor.predict_emotion(face_img, self.model)
                        if emotion is not None:
                            self.last_prediction = (emotion, confidence, largest_face)
                            self.last_prediction_time = current_time
            
            # V·∫Ω k·∫øt qu·∫£
            label = f'FPS: {self.current_fps:.1f}'
            if self.last_prediction is not None:
                emotion, confidence, (x, y, w, h) = self.last_prediction
                color = EMOTION_COLORS.get(emotion, (255, 255, 255))
                
                # V·∫Ω rectangle
                cv2.rectangle(output_img, (x, y), (x+w, y+h), color, 3)
                
                # V·∫Ω text v·ªõi background
                text = f"{emotion}: {confidence:.2f}"
                font = cv2.FONT_HERSHEY_SIMPLEX
                font_scale = 0.8
                thickness = 2
                
                (text_w, text_h), baseline = cv2.getTextSize(text, font, font_scale, thickness)
                
                # Background cho text
                bg_y = max(0, y - 10)
                cv2.rectangle(output_img, 
                            (x, bg_y - text_h - baseline), 
                            (x + text_w, bg_y), 
                            color, -1)
                
                # Text
                cv2.putText(output_img, text, (x, bg_y - baseline), 
                          font, font_scale, (255, 255, 255), thickness)
                
                label = f'{emotion}: {confidence:.2f} | FPS: {self.current_fps:.1f}'
            
            # Update session state
            if 'label' in st.session_state:
                st.session_state['label'] = label
            
            return av.VideoFrame.from_ndarray(output_img, format="bgr24")
            
        except Exception as e:
            logger.error(f"Video processing error: {e}")
            return frame

def process_uploaded_image(uploaded_file, model):
    """X·ª≠ l√Ω h√¨nh ·∫£nh ƒë∆∞·ª£c t·∫£i l√™n"""
    try:
        image = Image.open(uploaded_file)
        
        # Hi·ªÉn th·ªã h√¨nh ·∫£nh g·ªëc
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("üì∑ H√¨nh ·∫£nh g·ªëc")
            st.image(image, use_column_width=True)
        
        # X·ª≠ l√Ω
        img_array = np.array(image)
        if len(img_array.shape) == 3 and img_array.shape[2] == 3:
            img_array = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
        elif len(img_array.shape) == 3 and img_array.shape[2] == 4:
            img_array = cv2.cvtColor(img_array, cv2.COLOR_RGBA2BGR)
        
        gray = cv2.cvtColor(img_array, cv2.COLOR_BGR2GRAY)
        
        # Detect faces
        face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        faces = face_cascade.detectMultiScale(
            gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)
        )
        
        with col2:
            st.subheader("üé≠ K·∫øt qu·∫£ nh·∫≠n di·ªán")
            
            if len(faces) == 0:
                st.warning("‚ö†Ô∏è Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t trong h√¨nh ·∫£nh")
                return
            
            # X·ª≠ l√Ω t·∫•t c·∫£ c√°c khu√¥n m·∫∑t
            results = []
            result_img = img_array.copy()
            
            for i, (x, y, w, h) in enumerate(faces):
                face_img = img_array[y:y+h, x:x+w]
                emotion, confidence = ImageProcessor.predict_emotion(face_img, model)
                
                if emotion is not None:
                    results.append((emotion, confidence))
                    color = EMOTION_COLORS.get(emotion, (255, 255, 255))
                    
                    # V·∫Ω rectangle v√† text
                    cv2.rectangle(result_img, (x, y), (x+w, y+h), color, 3)
                    text = f"{emotion}: {confidence:.2f}"
                    cv2.putText(result_img, text, (x, y-10), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
            
            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            if results:
                result_img_rgb = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)
                st.image(result_img_rgb, use_column_width=True)
                
                st.subheader("üìä Chi ti·∫øt k·∫øt qu·∫£:")
                for i, (emotion, confidence) in enumerate(results):
                    st.write(f"**Khu√¥n m·∫∑t {i+1}:** {emotion} ({confidence:.2%})")
            else:
                st.error("‚ùå Kh√¥ng th·ªÉ d·ª± ƒëo√°n c·∫£m x√∫c t·ª´ c√°c khu√¥n m·∫∑t ƒë∆∞·ª£c ph√°t hi·ªán")
                
    except Exception as e:
        st.error(f"‚ùå L·ªói khi x·ª≠ l√Ω h√¨nh ·∫£nh: {e}")
        logger.error(f"Image processing error: {e}")

def main():
    # C·∫•u h√¨nh trang
    st.set_page_config(
        page_title="Emotion Recognition",
        page_icon="üé≠",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # Header
    st.title("üé≠ Nh·∫≠n Di·ªán C·∫£m X√∫c Khu√¥n M·∫∑t")
    st.markdown("---")
    
    # Sidebar th√¥ng tin
    with st.sidebar:
        st.header("‚ÑπÔ∏è Th√¥ng tin ·ª©ng d·ª•ng")
        st.info("""
        **M√¥ h√¨nh:** InceptionV3
        **C·∫£m x√∫c nh·∫≠n di·ªán:** 
        - üò† Angry (T·ª©c gi·∫≠n)
        - ü§¢ Disgust (Gh√™ t·ªüm)  
        - üò∞ Fear (S·ª£ h√£i)
        - üòä Happy (Vui v·∫ª)
        - üò¢ Sad (Bu·ªìn b√£)
        - üò≤ Surprise (Ng·∫°c nhi√™n)
        - üòê Neutral (B√¨nh th∆∞·ªùng)
        """)
        
        st.header("‚öôÔ∏è C√†i ƒë·∫∑t")
        st.write("·ª®ng d·ª•ng t·ª± ƒë·ªông t·ªëi ∆∞u hi·ªáu su·∫•t")
    
    # Kh·ªüi t·∫°o session state
    if 'label' not in st.session_state:
        st.session_state['label'] = 'üîÑ ƒêang kh·ªüi t·∫°o...'
    
    # T·∫£i m√¥ h√¨nh
    model = ModelManager.load_emotion_model()
    if model is None:
        st.error("‚ùå Kh√¥ng th·ªÉ ti·∫øp t·ª•c do l·ªói t·∫£i m√¥ h√¨nh.")
        st.stop()
    
    # Tabs cho c√°c ch·ª©c nƒÉng
    tab1, tab2 = st.tabs(["üì∑ T·∫£i h√¨nh ·∫£nh", "üé• Camera tr·ª±c ti·∫øp"])
    
    with tab1:
        st.subheader("üì§ T·∫£i h√¨nh ·∫£nh ƒë·ªÉ d·ª± ƒëo√°n c·∫£m x√∫c")
        st.write("H·ªó tr·ª£ ƒë·ªãnh d·∫°ng: JPG, JPEG, PNG")
        
        uploaded_file = st.file_uploader(
            "Ch·ªçn h√¨nh ·∫£nh", 
            type=["jpg", "jpeg", "png"],
            help="H√¨nh ·∫£nh n√™n ch·ª©a khu√¥n m·∫∑t r√µ r√†ng ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªët nh·∫•t"
        )
        
        if uploaded_file is not None:
            process_uploaded_image(uploaded_file, model)
    
    with tab2:
        st.subheader("üé• Nh·∫≠n di·ªán c·∫£m x√∫c t·ª´ camera")
        st.write("Cho ph√©p truy c·∫≠p camera v√† b·∫Øt ƒë·∫ßu nh·∫≠n di·ªán")
        
        # C·∫•u h√¨nh WebRTC
        RTC_CONFIGURATION = RTCConfiguration({
            "iceServers": [
                {"urls": ["stun:stun.l.google.com:19302"]},
                {"urls": ["stun:stun1.l.google.com:19302"]},
            ]
        })
        
        # WebRTC Streamer
        webrtc_ctx = webrtc_streamer(
            key="emotion-recognition",
            video_processor_factory=lambda: OptimizedVideoProcessor(model),
            rtc_configuration=RTC_CONFIGURATION,
            media_stream_constraints={
                "video": {
                    "width": {"ideal": 640},
                    "height": {"ideal": 480},
                    "frameRate": {"ideal": 15, "max": 30}
                }, 
                "audio": False
            },
            async_processing=True,
        )
        
        # Hi·ªÉn th·ªã tr·∫°ng th√°i
        status_placeholder = st.empty()
        
        if webrtc_ctx.video_processor:
            with status_placeholder.container():
                st.success("‚úÖ Camera ƒëang ho·∫°t ƒë·ªông")
                if 'label' in st.session_state:
                    st.info(f"üìä **Tr·∫°ng th√°i:** {st.session_state['label']}")
        else:
            with status_placeholder.container():
                st.warning("‚ö†Ô∏è Camera ch∆∞a ƒë∆∞·ª£c k√≠ch ho·∫°t. Nh·∫•n 'Start' ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
    
    # Footer
    st.markdown("---")
    st.markdown(
        "<div style='text-align: center; color: gray;'>"
        "üé≠ Emotion Recognition App | Powered by L√™ Ph·ª•ng"
        "</div>", 
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()
