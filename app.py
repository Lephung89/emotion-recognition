import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration
import cv2
import numpy as np
from tensorflow.keras.models import load_model
import av
import time
from PIL import Image
import os
import gdown
import threading
import queue
from pathlib import Path
import logging

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Constants
MODEL_PATH = "best_modelnew.h5.keras"
GOOGLE_DRIVE_URL = "https://drive.google.com/uc?id=1nB_Sr_jnm0HmMSC4ISf2bFOYPGLW15Bc"
EMOTION_LABELS = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']
EMOTION_COLORS = {
    'angry': (0, 0, 255),      # Red
    'disgust': (0, 255, 255),  # Yellow
    'fear': (128, 0, 128),     # Purple
    'happy': (0, 128, 0),      # Green
    'sad': (255, 0, 0),        # Blue
    'surprise': (255, 165, 0), # Orange
    'neutral': (128, 128, 128) # Gray
}

class ModelManager:
    """Qu·∫£n l√Ω t·∫£i v√† cache m√¥ h√¨nh"""
    
    @staticmethod
    @st.cache_resource
    def load_emotion_model_from_file(model_path: str):
        """T·∫£i m√¥ h√¨nh t·ª´ file path c·ª• th·ªÉ"""
        try:
            model = load_model(model_path, compile=False)
            logger.info(f"Model loaded successfully from: {model_path}")
            return model
        except Exception as e:
            logger.error(f"Model loading error: {e}")
            raise e
    
    @staticmethod
    def check_and_load_model():
        """Ki·ªÉm tra v√† t·∫£i m√¥ h√¨nh v·ªõi c√°c ph∆∞∆°ng th·ª©c kh√°c nhau"""
        model_path = Path(MODEL_PATH)
        
        # Ki·ªÉm tra file t·ªìn t·∫°i v√† t·∫£i tr·ª±c ti·∫øp
        if model_path.exists():
            try:
                model = ModelManager.load_emotion_model_from_file(str(model_path))
                st.success(f"‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng t·ª´: {model_path}")
                return model
            except Exception as e:
                st.error(f"‚ùå L·ªói khi t·∫£i m√¥ h√¨nh t·ª´ file c√≥ s·∫µn: {e}")
                # X√≥a file l·ªói
                try:
                    model_path.unlink()
                except:
                    pass
        
        # T·∫£i t·ª´ Google Drive
        return ModelManager._download_from_drive()
    
    @staticmethod
    def _download_from_drive():
        """T·∫£i m√¥ h√¨nh t·ª´ Google Drive"""
        try:
            with st.spinner("üîÑ ƒêang t·∫£i m√¥ h√¨nh t·ª´ Google Drive..."):
                gdown.download(GOOGLE_DRIVE_URL, MODEL_PATH, quiet=False)
                
            if Path(MODEL_PATH).exists():
                model = ModelManager.load_emotion_model_from_file(MODEL_PATH)
                st.success("‚úÖ T·∫£i m√¥ h√¨nh t·ª´ Google Drive th√†nh c√¥ng!")
                return model
            else:
                raise FileNotFoundError("Kh√¥ng th·ªÉ t·∫£i file t·ª´ Google Drive.")
                
        except Exception as e:
            st.error(f"‚ùå L·ªói khi t·∫£i t·ª´ Google Drive: {e}")
            st.info("üí° B·∫°n c√≥ th·ªÉ th·ª≠ c√°c c√°ch sau:")
            st.markdown("""
            1. **Ki·ªÉm tra k·∫øt n·ªëi internet**
            2. **ƒê·∫£m b·∫£o Google Drive link h·ª£p l·ªá**
            3. **T·∫£i file m√¥ h√¨nh th·ªß c√¥ng ·ªü ph·∫ßn d∆∞·ªõi**
            """)
            return None
    
    @staticmethod
    def handle_manual_upload(uploaded_model):
        """X·ª≠ l√Ω t·∫£i m√¥ h√¨nh th·ªß c√¥ng"""
        if uploaded_model is not None:
            try:
                # L∆∞u file t·∫°m th·ªùi
                temp_path = f"temp_{MODEL_PATH}"
                with open(temp_path, "wb") as f:
                    f.write(uploaded_model.getbuffer())
                
                # Th·ª≠ t·∫£i m√¥ h√¨nh
                model = ModelManager.load_emotion_model_from_file(temp_path)
                
                # N·∫øu th√†nh c√¥ng, chuy·ªÉn sang t√™n ch√≠nh th·ª©c
                if Path(MODEL_PATH).exists():
                    Path(MODEL_PATH).unlink()
                Path(temp_path).rename(MODEL_PATH)
                
                st.success("‚úÖ File m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i l√™n v√† kh·ªüi t·∫°o th√†nh c√¥ng!")
                st.balloons()
                return model
                
            except Exception as e:
                st.error(f"‚ùå L·ªói khi x·ª≠ l√Ω file m√¥ h√¨nh: {e}")
                # X√≥a file t·∫°m n·∫øu l·ªói
                if Path(f"temp_{MODEL_PATH}").exists():
                    Path(f"temp_{MODEL_PATH}").unlink()
                return None
        
        return None

class ImageProcessor:
    """X·ª≠ l√Ω h√¨nh ·∫£nh v√† d·ª± ƒëo√°n c·∫£m x√∫c"""
    
    @staticmethod
    def preprocess_face(face_img):
        """Ti·ªÅn x·ª≠ l√Ω ·∫£nh khu√¥n m·∫∑t v·ªõi error handling"""
        try:
            if face_img is None or face_img.size == 0:
                return None
                
            # Chuy·ªÉn ƒë·ªïi m√†u s·∫Øc
            if len(face_img.shape) == 3 and face_img.shape[2] == 3:
                face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)
            
            # C·∫£i thi·ªán ƒë·ªô t∆∞∆°ng ph·∫£n
            lab = cv2.cvtColor(face_img, cv2.COLOR_RGB2LAB)
            l, a, b = cv2.split(lab)
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            l = clahe.apply(l)
            lab = cv2.merge((l, a, b))
            face_img = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
            
            # Resize v√† normalize
            face_img = cv2.resize(face_img, (299, 299))
            face_img = face_img.astype('float32')
            face_img = (face_img - 127.5) / 127.5
            face_img = np.expand_dims(face_img, axis=0)
            
            return face_img
        except Exception as e:
            logger.error(f"Preprocessing error: {e}")
            return None
    
    @staticmethod
    def predict_emotion(face_img, model):
        """D·ª± ƒëo√°n c·∫£m x√∫c v·ªõi confidence threshold"""
        try:
            if model is None:
                return None, 0.0
                
            processed_img = ImageProcessor.preprocess_face(face_img)
            if processed_img is None:
                return None, 0.0
            
            predictions = model.predict(processed_img, verbose=0)
            predicted_class = np.argmax(predictions[0])
            confidence = float(predictions[0][predicted_class])
            
            # Ch·ªâ tr·∫£ v·ªÅ k·∫øt qu·∫£ n·∫øu confidence > threshold
            if confidence > 0.3:  # Threshold ƒë·ªÉ l·ªçc prediction y·∫øu
                return EMOTION_LABELS[predicted_class], confidence
            else:
                return None, confidence
                
        except Exception as e:
            logger.error(f"Prediction error: {e}")
            return None, 0.0

class OptimizedVideoProcessor(VideoProcessorBase):
    """Video processor ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a v·ªõi x·ª≠ l√Ω no-face detection"""
    
    def __init__(self, model):
        self.model = model
        self.face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        
        # Performance tracking
        self.fps_counter = 0
        self.fps_start_time = time.time()
        self.current_fps = 0
        
        # Prediction caching v√† tracking
        self.last_prediction = None
        self.last_prediction_time = 0
        self.prediction_interval = 0.3
        self.no_face_start_time = None  # Th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu kh√¥ng c√≥ face
        self.no_face_threshold = 1.0    # Sau 1 gi√¢y kh√¥ng c√≥ face th√¨ clear prediction
        
        # Frame skipping for better performance
        self.frame_count = 0
        self.process_every_n_frames = 3
        
        logger.info("VideoProcessor initialized successfully")
    
    def recv(self, frame):
        try:
            img = frame.to_ndarray(format="bgr24")
            self.frame_count += 1
            current_time = time.time()
            
            # T√≠nh FPS
            self.fps_counter += 1
            if current_time - self.fps_start_time >= 1.0:
                self.current_fps = self.fps_counter / (current_time - self.fps_start_time)
                self.fps_counter = 0
                self.fps_start_time = current_time
            
            output_img = img.copy()
            
            # LU√îN LU√îN ki·ªÉm tra face detection ƒë·ªÉ responsive h∆°n
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            
            # Detect faces v·ªõi parameters t·ªëi ∆∞u
            faces = self.face_cascade.detectMultiScale(
                gray, 
                scaleFactor=1.1, 
                minNeighbors=3, 
                minSize=(60, 60),
                flags=cv2.CASCADE_SCALE_IMAGE
            )
            
            faces_detected = len(faces) > 0
            
            if faces_detected:
                # C√ì FACE - Reset no-face timer
                self.no_face_start_time = None
                
                # Ch·ªâ predict emotion theo interval ƒë·ªÉ t·ªëi ∆∞u performance
                should_process = (self.frame_count % self.process_every_n_frames == 0)
                should_predict = (current_time - self.last_prediction_time) >= self.prediction_interval
                
                if should_process and should_predict:
                    # Ch·ªçn khu√¥n m·∫∑t l·ªõn nh·∫•t (g·∫ßn camera nh·∫•t)
                    largest_face = max(faces, key=lambda face: face[2] * face[3])
                    x, y, w, h = largest_face
                    
                    # M·ªü r·ªông v√πng face m·ªôt ch√∫t
                    margin = int(max(w, h) * 0.1)
                    face_x = max(0, x - margin)
                    face_y = max(0, y - margin)
                    face_w = min(img.shape[1] - face_x, w + 2 * margin)
                    face_h = min(img.shape[0] - face_y, h + 2 * margin)
                    
                    face_img = img[face_y:face_y+face_h, face_x:face_x+face_w]
                    
                    if face_img.size > 0:
                        emotion, confidence = ImageProcessor.predict_emotion(face_img, self.model)
                        if emotion is not None:
                            self.last_prediction = (emotion, confidence, largest_face)
                            self.last_prediction_time = current_time
            else:
                # KH√îNG C√ì FACE
                if self.no_face_start_time is None:
                    # B·∫Øt ƒë·∫ßu ƒë·∫øm th·ªùi gian kh√¥ng c√≥ face
                    self.no_face_start_time = current_time
                elif (current_time - self.no_face_start_time) > self.no_face_threshold:
                    # ƒê√£ qu√° l√¢u kh√¥ng c√≥ face, X√ìA prediction c≈©
                    self.last_prediction = None
            
            # V·∫Ω k·∫øt qu·∫£
            if faces_detected or (self.last_prediction is not None and 
                                 self.no_face_start_time is None):
                # C√≥ face hi·ªán t·∫°i ho·∫∑c v·ª´a m·ªõi m·∫•t face (ch∆∞a qu√° threshold)
                if self.last_prediction is not None:
                    emotion, confidence, (x, y, w, h) = self.last_prediction
                    color = EMOTION_COLORS.get(emotion, (255, 255, 255))
                    
                    # V·∫Ω rectangle
                    cv2.rectangle(output_img, (x, y), (x+w, y+h), color, 3)
                    
                    # V·∫Ω text v·ªõi background
                    text = f"{emotion}: {confidence:.2f}"
                    font = cv2.FONT_HERSHEY_SIMPLEX
                    font_scale = 0.8
                    thickness = 2
                    
                    (text_w, text_h), baseline = cv2.getTextSize(text, font, font_scale, thickness)
                    
                    # Background cho text
                    bg_y = max(0, y - 10)
                    cv2.rectangle(output_img, 
                                (x, bg_y - text_h - baseline), 
                                (x + text_w, bg_y), 
                                color, -1)
                    
                    # Text
                    cv2.putText(output_img, text, (x, bg_y - baseline), 
                              font, font_scale, (255, 255, 255), thickness)
                    
                    label = f'{emotion}: {confidence:.2f} | FPS: {self.current_fps:.1f}'
                else:
                    label = f'FPS: {self.current_fps:.1f}'
            else:
                # Kh√¥ng c√≥ face ƒë∆∞·ª£c ph√°t hi·ªán
                # Hi·ªÉn th·ªã th√¥ng b√°o "Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t"
                text = "no face detected"
                font = cv2.FONT_HERSHEY_SIMPLEX
                font_scale = 1.0
                thickness = 2
                color = (0, 0, 255)  # M√†u ƒë·ªè
                
                # T√≠nh to√°n v·ªã tr√≠ ƒë·ªÉ ƒë·∫∑t text ·ªü gi·ªØa m√†n h√¨nh
                (text_w, text_h), baseline = cv2.getTextSize(text, font, font_scale, thickness)
                img_h, img_w = output_img.shape[:2]
                text_x = (img_w - text_w) // 2
                text_y = (img_h + text_h) // 2
                
                # Background cho text
                cv2.rectangle(output_img, 
                            (text_x - 10, text_y - text_h - baseline - 10), 
                            (text_x + text_w + 10, text_y + baseline + 10), 
                            (0, 0, 0), -1)
                
                # Text
                cv2.putText(output_img, text, (text_x, text_y), 
                          font, font_scale, color, thickness)
                
                label = f'Khong phat hien khuon mat | FPS: {self.current_fps:.1f}'
            
            # Update session state
            if 'label' in st.session_state:
                st.session_state['label'] = label
            
            return av.VideoFrame.from_ndarray(output_img, format="bgr24")
            
        except Exception as e:
            logger.error(f"Video processing error: {e}")
            return frame

def get_webrtc_configuration():
    """C·∫•u h√¨nh WebRTC t·ªëi ∆∞u ƒë·ªÉ kh·∫Øc ph·ª•c l·ªói connection"""
    
    # C·∫•u h√¨nh STUN servers m·∫°nh m·∫Ω h∆°n
    ice_servers = [
        # Google STUN servers - ·ªîn ƒë·ªãnh nh·∫•t
        {"urls": "stun:stun.l.google.com:19302"},
        {"urls": "stun:stun1.l.google.com:19302"},
        {"urls": "stun:stun2.l.google.com:19302"},
        
        # OpenRelay TURN servers - Free v√† ·ªïn ƒë·ªãnh
        {
            "urls": "turn:relay1.expressturn.com:3480",
            "username": "000000002063846457",
            "credential": "IjDTiMpkfaNqnGhlGFSRC7GMJpU="
        },
      
        
        # Backup STUN servers
        {"urls": "stun:stun.stunprotocol.org:3478"},
        {"urls": "stun:stun.mozilla.org"},
    ]
    
    return RTCConfiguration({
        "iceServers": ice_servers,
        "iceTransportPolicy": "all",
        "bundlePolicy": "max-bundle",
        "rtcpMuxPolicy": "require", 
        "iceCandidatePoolSize": 10,
        "iceConnectionReceiveTimeout": 30000,  # 30 seconds
        "iceGatheringTimeout": 10000,         # 10 seconds
    })
def process_uploaded_image(uploaded_file, model):
    """X·ª≠ l√Ω h√¨nh ·∫£nh ƒë∆∞·ª£c t·∫£i l√™n"""
    try:
        image = Image.open(uploaded_file)
        
        # Hi·ªÉn th·ªã h√¨nh ·∫£nh g·ªëc
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("üì∑ H√¨nh ·∫£nh g·ªëc")
            st.image(image, use_column_width=True)
        
        # X·ª≠ l√Ω
        img_array = np.array(image)
        if len(img_array.shape) == 3 and img_array.shape[2] == 3:
            img_array = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
        elif len(img_array.shape) == 3 and img_array.shape[2] == 4:
            img_array = cv2.cvtColor(img_array, cv2.COLOR_RGBA2BGR)
        
        gray = cv2.cvtColor(img_array, cv2.COLOR_BGR2GRAY)
        
        # Detect faces
        face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        faces = face_cascade.detectMultiScale(
            gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)
        )
        
        with col2:
            st.subheader("üé≠ K·∫øt qu·∫£ nh·∫≠n di·ªán")
            
            if len(faces) == 0:
                st.warning("‚ö†Ô∏è Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t trong h√¨nh ·∫£nh")
                return
            
            # X·ª≠ l√Ω t·∫•t c·∫£ c√°c khu√¥n m·∫∑t
            results = []
            result_img = img_array.copy()
            
            for i, (x, y, w, h) in enumerate(faces):
                face_img = img_array[y:y+h, x:x+w]
                emotion, confidence = ImageProcessor.predict_emotion(face_img, model)
                
                if emotion is not None:
                    results.append((emotion, confidence))
                    color = EMOTION_COLORS.get(emotion, (255, 255, 255))
                    
                    # V·∫Ω rectangle v√† text
                    cv2.rectangle(result_img, (x, y), (x+w, y+h), color, 3)
                    text = f"{emotion}: {confidence:.2f}"
                    cv2.putText(result_img, text, (x, y-10), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
            
            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            if results:
                result_img_rgb = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)
                st.image(result_img_rgb, use_column_width=True)
                
                st.subheader("üìä Chi ti·∫øt k·∫øt qu·∫£:")
                for i, (emotion, confidence) in enumerate(results):
                    st.write(f"**Khu√¥n m·∫∑t {i+1}:** {emotion} ({confidence:.2%})")
            else:
                st.error("‚ùå Kh√¥ng th·ªÉ d·ª± ƒëo√°n c·∫£m x√∫c t·ª´ c√°c khu√¥n m·∫∑t ƒë∆∞·ª£c ph√°t hi·ªán")
                
    except Exception as e:
        st.error(f"‚ùå L·ªói khi x·ª≠ l√Ω h√¨nh ·∫£nh: {e}")
        logger.error(f"Image processing error: {e}")

def main():
    # C·∫•u h√¨nh trang
    st.set_page_config(
        page_title="Emotion Recognition",
        page_icon="üé≠",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    def hide_streamlit_style():
    """·∫®n menu v√† c√°c th√†nh ph·∫ßn kh√¥ng c·∫ßn thi·∫øt c·ªßa Streamlit"""
    hide_st_style = """
            <style>
            /* ·∫®n menu hamburger */
            #MainMenu {visibility: hidden;}
            
            /* ·∫®n header m·∫∑c ƒë·ªãnh */
            header {visibility: hidden;}
            
            /* ·∫®n footer "Made with Streamlit" */
            footer {visibility: hidden;}
            
            /* ·∫®n n√∫t Deploy (n·∫øu c√≥) */
            .stDeployButton {display: none;}
            
            /* ·∫®n to√†n b·ªô toolbar ph√≠a tr√™n */
            .stAppToolbar {display: none;}
            
            /* T√πy ch·ªçn: ·∫®n ph·∫ßn padding ph√≠a tr√™n */
            .stAppHeader {display: none;}
            
            /* T√πy ch·ªçn: ƒêi·ªÅu ch·ªânh padding */
            .main .block-container {
                padding-top: 1rem;
                padding-bottom: 0rem;
            }
            </style>
            """
    st.markdown(hide_st_style, unsafe_allow_html=True)
    # Header
    st.title("üé≠ Nh·∫≠n Di·ªán C·∫£m X√∫c Khu√¥n M·∫∑t")
    st.markdown("---")
    
    # Sidebar th√¥ng tin
    with st.sidebar:
        st.header("‚ÑπÔ∏è Th√¥ng tin ·ª©ng d·ª•ng")
        st.info("""
        **M√¥ h√¨nh:**
        **C·∫£m x√∫c nh·∫≠n di·ªán:** 
        - üò† Angry (T·ª©c gi·∫≠n)
        - ü§¢ Disgust (Gh√™ t·ªüm)  
        - üò∞ Fear (S·ª£ h√£i)
        - üòä Happy (Vui v·∫ª)
        - üò¢ Sad (Bu·ªìn b√£)
        - üò≤ Surprise (Ng·∫°c nhi√™n)
        - üòê Neutral (B√¨nh th∆∞·ªùng)
        """)
        
        st.header("‚öôÔ∏è C√†i ƒë·∫∑t")
        if st.button("üîÑ T·∫£i l·∫°i m√¥ h√¨nh", help="X√≥a cache v√† t·∫£i l·∫°i m√¥ h√¨nh"):
            st.cache_resource.clear()
            st.rerun()
        
        # C√†i ƒë·∫∑t WebRTC
        st.header("üì° C√†i ƒë·∫∑t Camera")
        video_quality = st.selectbox(
            "Ch·∫•t l∆∞·ª£ng video",
            ["Th·∫•p (320x240)", "Trung b√¨nh (640x480)", "Cao (1280x720)"],
            index=1,
            help="Ch·∫•t l∆∞·ª£ng th·∫•p h∆°n gi√∫p k·∫øt n·ªëi ·ªïn ƒë·ªãnh h∆°n tr√™n m·∫°ng ch·∫≠m"
        )
        
        frame_rate = st.slider(
            "T·ªëc ƒë·ªô khung h√¨nh (FPS)",
            min_value=5,
            max_value=30,
            value=15,
            help="FPS th·∫•p h∆°n gi√∫p k·∫øt n·ªëi ·ªïn ƒë·ªãnh h∆°n"
        )
        
        # Network troubleshooting
        st.header("üîß Kh·∫Øc ph·ª•c s·ª± c·ªë")
        if st.button("üîç Ki·ªÉm tra k·∫øt n·ªëi"):
            st.info("""
            **N·∫øu g·∫∑p l·ªói k·∫øt n·ªëi:**
            1. Th·ª≠ chuy·ªÉn sang ch·∫•t l∆∞·ª£ng video th·∫•p h∆°n
            2. Gi·∫£m FPS xu·ªëng 10-15
            3. Ki·ªÉm tra t∆∞·ªùng l·ª≠a c·ªßa c√¥ng ty/tr∆∞·ªùng h·ªçc
            4. Th·ª≠ d√πng VPN n·∫øu m·∫°ng b·ªã h·∫°n ch·∫ø
            5. Restart browser v√† clear cache
            """)
        
        # Hi·ªÉn th·ªã tr·∫°ng th√°i model
        if Path(MODEL_PATH).exists():
            st.success("‚úÖ M√¥ h√¨nh c√≥ s·∫µn")
        else:
            st.warning("‚ö†Ô∏è Ch∆∞a c√≥ m√¥ h√¨nh")
    
    # Kh·ªüi t·∫°o session state
    if 'label' not in st.session_state:
        st.session_state['label'] = 'üîÑ ƒêang kh·ªüi t·∫°o...'
    if 'model_loaded' not in st.session_state:
        st.session_state['model_loaded'] = False
    
    # T·∫£i m√¥ h√¨nh
    model = ModelManager.check_and_load_model()
    
    # N·∫øu kh√¥ng c√≥ model, hi·ªÉn th·ªã ph·∫ßn upload
    if model is None:
        st.subheader("üì§ T·∫£i m√¥ h√¨nh th·ªß c√¥ng")
        st.info("Vui l√≤ng t·∫£i file m√¥ h√¨nh (.h5 ho·∫∑c .keras) ƒë·ªÉ ti·∫øp t·ª•c.")
        
        uploaded_model = st.file_uploader(
            "Ch·ªçn file m√¥ h√¨nh", 
            type=["h5", "keras"],
            help="File m√¥ h√¨nh ph·∫£i c√≥ ƒë·ªãnh d·∫°ng .h5 ho·∫∑c .keras",
            key="model_uploader"
        )
        
        if uploaded_model is not None:
            with st.spinner("üîÑ ƒêang x·ª≠ l√Ω file m√¥ h√¨nh..."):
                model = ModelManager.handle_manual_upload(uploaded_model)
                if model is not None:
                    st.session_state['model_loaded'] = True
                    st.rerun()
    
    if model is None:
        st.error("‚ùå Kh√¥ng th·ªÉ ti·∫øp t·ª•c do ch∆∞a c√≥ m√¥ h√¨nh. Vui l√≤ng t·∫£i m√¥ h√¨nh ·ªü tr√™n.")
        st.stop()
    
    # Tabs cho c√°c ch·ª©c nƒÉng
    tab1, tab2 = st.tabs(["üì∑ T·∫£i h√¨nh ·∫£nh", "üé• Camera tr·ª±c ti·∫øp"])
    
    with tab1:
        st.subheader("üì§ T·∫£i h√¨nh ·∫£nh ƒë·ªÉ d·ª± ƒëo√°n c·∫£m x√∫c")
        st.write("H·ªó tr·ª£ ƒë·ªãnh d·∫°ng: JPG, JPEG, PNG")
        
        uploaded_file = st.file_uploader(
            "Ch·ªçn h√¨nh ·∫£nh", 
            type=["jpg", "jpeg", "png"],
            help="H√¨nh ·∫£nh n√™n ch·ª©a khu√¥n m·∫∑t r√µ r√†ng ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªët nh·∫•t",
            key="image_uploader"
        )
        
        if uploaded_file is not None:
            process_uploaded_image(uploaded_file, model)
    
    with tab2:
        st.subheader("üé• Nh·∫≠n di·ªán c·∫£m x√∫c t·ª´ camera")
        st.write("Cho ph√©p truy c·∫≠p camera v√† b·∫Øt ƒë·∫ßu nh·∫≠n di·ªán")
        
        # C·∫•u h√¨nh video d·ª±a tr√™n l·ª±a ch·ªçn ng∆∞·ªùi d√πng
        if video_quality == "Th·∫•p (320x240)":
            video_constraints = {
                "width": {"ideal": 320},
                "height": {"ideal": 240},
            }
        elif video_quality == "Trung b√¨nh (640x480)":
            video_constraints = {
                "width": {"ideal": 640},
                "height": {"ideal": 480},
            }
        else:  # Cao
            video_constraints = {
                "width": {"ideal": 1280},
                "height": {"ideal": 720},
            }
        
        video_constraints["frameRate"] = {"ideal": frame_rate, "max": 30}
        
        # C·∫£nh b√°o v·ªÅ m·∫°ng
        st.warning("""
        ‚ö†Ô∏è **L∆∞u √Ω v·ªÅ k·∫øt n·ªëi:**
        - N·∫øu g·∫∑p l·ªói "connection taking longer", h√£y th·ª≠ gi·∫£m ch·∫•t l∆∞·ª£ng video
        - M·ªôt s·ªë m·∫°ng c√¥ng ty/tr∆∞·ªùng h·ªçc c√≥ th·ªÉ ch·∫∑n WebRTC
        - M·∫°ng 4G ƒë√¥i khi kh√¥ng ·ªïn ƒë·ªãnh v·ªõi WebRTC
        """)
        
        # WebRTC Streamer v·ªõi c·∫•u h√¨nh c·∫£i thi·ªán
        webrtc_ctx = webrtc_streamer(
            key="emotion-recognition-improved",
            video_processor_factory=lambda: OptimizedVideoProcessor(model),
            rtc_configuration=get_webrtc_configuration(),
            media_stream_constraints={
                "video": video_constraints,
                "audio": False
            },
            async_processing=True,
        )
        
        # Hi·ªÉn th·ªã tr·∫°ng th√°i
        status_placeholder = st.empty()
        
        if webrtc_ctx.video_processor:
            with status_placeholder.container():
                st.success("‚úÖ Camera ƒëang ho·∫°t ƒë·ªông")
                if 'label' in st.session_state:
                    st.info(f"üìä **Tr·∫°ng th√°i:** {st.session_state['label']}")
        else:
            with status_placeholder.container():
                st.warning("‚ö†Ô∏è Camera ch∆∞a ƒë∆∞·ª£c k√≠ch ho·∫°t. Nh·∫•n 'Start' ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
                
                # Th√™m h∆∞·ªõng d·∫´n troubleshooting
                with st.expander("üîß H∆∞·ªõng d·∫´n kh·∫Øc ph·ª•c l·ªói k·∫øt n·ªëi"):
                    st.markdown("""
                    ### C√°c b∆∞·ªõc kh·∫Øc ph·ª•c:
                    
                    1. **Ki·ªÉm tra quy·ªÅn truy c·∫≠p camera:**
                       - ƒê·∫£m b·∫£o browser c√≥ quy·ªÅn truy c·∫≠p camera
                       - Ki·ªÉm tra bi·ªÉu t∆∞·ª£ng camera tr√™n thanh ƒë·ªãa ch·ªâ
                       
                    2. **Th·ª≠ c√°c c√†i ƒë·∫∑t kh√°c nhau:**
                       - Gi·∫£m ch·∫•t l∆∞·ª£ng video xu·ªëng "Th·∫•p"
                       - Gi·∫£m FPS xu·ªëng 10-15
                       - Refresh trang v√† th·ª≠ l·∫°i
                       
                    3. **V·∫•n ƒë·ªÅ m·∫°ng:**
                       - Th·ª≠ ƒë·ªïi sang m·∫°ng WiFi kh√°c
                       - T·∫Øt VPN n·∫øu ƒëang s·ª≠ d·ª•ng
                       - Ki·ªÉm tra t∆∞·ªùng l·ª≠a c√¥ng ty/tr∆∞·ªùng h·ªçc
                       
                    4. **Browser issues:**
                       - Th·ª≠ browser kh√°c (Chrome, Firefox, Edge)
                       - Clear cache v√† cookies
                       - T·∫Øt extensions c√≥ th·ªÉ can thi·ªáp
                       
                    5. **N·∫øu v·∫´n kh√¥ng ƒë∆∞·ª£c:**
                       - S·ª≠ d·ª•ng ch·ª©c nƒÉng "T·∫£i h√¨nh ·∫£nh" thay th·∫ø
                       - Li√™n h·ªá IT support n·∫øu ·ªü m√¥i tr∆∞·ªùng c√¥ng ty
                    """)
    
    # Footer
    st.markdown("---")
    st.markdown(
        "<div style='text-align: center; color: gray;'>"
        "üé≠ Emotion Recognition App | Powered by L√™ Ph·ª•ng<br>"
        "üí° Tip: N·∫øu g·∫∑p v·∫•n ƒë·ªÅ k·∫øt n·ªëi, h√£y th·ª≠ gi·∫£m ch·∫•t l∆∞·ª£ng video ho·∫∑c s·ª≠ d·ª•ng ch·ª©c nƒÉng t·∫£i ·∫£nh"
        "</div>", 
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()
